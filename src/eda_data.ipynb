{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection of Scripts for Exploratory Data Analysis\n",
    "Author: Johanna Garthe\n",
    "+ Tweet volume timeline\n",
    "+ Dataset distribution\n",
    "+ Max sequence length\n",
    "+ Under-sampling the majority class at random\n",
    "+ Check unique number of users per dataset\n",
    "+ Check for text duplicates written by same user\n",
    "+ Delete duplicates by text_ID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet volume timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dir = \" \"\n",
    "fileNames = os.listdir(dir)\n",
    "fileNames = [file for file in fileNames if '.csv' in file]\n",
    "for file in fileNames:\n",
    "    df = pd.read_csv(dir + file, \\\n",
    "        parse_dates=['created_at'])\n",
    "    #df_tweets = df.loc[df['text_type'] == 'tweet']\n",
    "    df_grouped = df.groupby(pd.Grouper(key='created_at',freq='1D', convention='start')).size()\n",
    "    df_period = df_grouped.loc['2022-02-24' : '2022-12-31']\n",
    "    df_period.plot(figsize=(20,10), label=(file.rsplit('.', 1)[0]))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Date of Tweet Creation', fontsize=21)\n",
    "plt.ylabel('1 Day Tweet Count\\n', fontsize=21)\n",
    "plt.title('Tweet Volume Timeline of Target US\\n', fontsize=24)\n",
    "plt.style.use('fivethirtyeight')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.gca().get_lines()[0].set_color(\"#fc4f30\")\n",
    "plt.gca().get_lines()[1].set_color(\"#008fd5\")\n",
    "order =[0,1]\n",
    "plt.gca().legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='center left', bbox_to_anchor=(1, 0.5), fontsize=18, title_fontsize=21, title='Datasets')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "file_scraped_data = \" \"\n",
    "df = pd.read_csv(file_scraped_data)\n",
    "total_count = df['target'].value_counts()\n",
    "favor = df.loc[df['label'] == 'FAVOR']\n",
    "favor_count = favor[\"target\"].value_counts()\n",
    "against = df.loc[df['label'] == 'AGAINST']\n",
    "against_count = against[\"target\"].value_counts()\n",
    "df_allcounts = pd.DataFrame({'Target':total_count.index, 'Total':total_count.values})\n",
    "df_favcounts = pd.DataFrame({'Target':favor_count.index, 'Favor':favor_count.values})\n",
    "df_agcounts = pd.DataFrame({'Target':against_count.index, 'Against':against_count.values})\n",
    "dfs = [df_allcounts, df_agcounts, df_favcounts]\n",
    "allcounts = reduce(lambda  left, right: pd.merge(left,right,on=['Target'],how='outer'), dfs)\n",
    "replace_values = {'ukraine support' : 'US','npps operation continuation' : 'NOC','arms delivery' : 'AD','speed limit implementation':'SLI'}\n",
    "allcounts = allcounts.replace({\"Target\": replace_values}).sort_values('Target')\n",
    "allcounts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "file = \" \"\n",
    "data = pd.read_csv(file)\n",
    "model_ckpt = \" \"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "token_lens = []\n",
    "for txt in data.text_cleaned:\n",
    "  tokens = tokenizer.encode(txt, truncation=True, max_length=512)\n",
    "  token_lens.append(len(tokens))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel('Token count')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#df = pd.DataFrame(token_lens)\n",
    "#df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-sampling the majority class at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "file_name = \" \"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "\"\"\" Downsampling of Class Arms Delivery - Favor \"\"\"\n",
    "print('********* Downsampling of Class Arms Delivery - Favor *********')\n",
    "arms_against = df[(df['target']=='arms delivery') & (df['label']=='FAVOR')]\n",
    "print(arms_against.shape)\n",
    "arms_against_downsample = resample(arms_against,\n",
    "                replace=False,\n",
    "                n_samples=5500)\n",
    "print(arms_against_downsample.shape)\n",
    "\n",
    "\"\"\" Downsampling of Class NPPs Operation Continuation - Against \"\"\"\n",
    "print('********* Downsampling of Class NPPs Operation Continuation - Against *********')\n",
    "npps_against = df[(df['target']=='npps operation continuation') & (df['label']=='AGAINST')]\n",
    "print(npps_against.shape)\n",
    "npps_against_downsample = resample(npps_against,\n",
    "                replace=False,\n",
    "                n_samples=5500)\n",
    "print(npps_against_downsample.shape)\n",
    "\n",
    "\"\"\" Downsampling of Class Speedlimit Implementation - Favor \"\"\"\n",
    "print('********* Downsampling of Class Speedlimit Implementation - Favor *********')\n",
    "speed_favor= df[(df['target']=='speed limit implementation') & (df['label']=='FAVOR')]\n",
    "print(speed_favor.shape)\n",
    "speed_favor_downsample = resample(speed_favor,\n",
    "                replace=False,\n",
    "                n_samples=5000)\n",
    "print(speed_favor_downsample.shape)\n",
    "\n",
    "\"\"\" Downsampling of Class Ukraine Support - Favor \"\"\"\n",
    "print('********* Downsampling of Class Ukraine Support - Favor *********')\n",
    "ua_favor = df[(df['target']=='ukraine support') & (df['label']=='FAVOR')]\n",
    "print(ua_favor.shape)\n",
    "ua_favor_downsample = resample(ua_favor,\n",
    "                replace=False,\n",
    "                n_samples=5500,)\n",
    "print(ua_favor_downsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete majority class rows of the original dataset dataframe and save in a new\n",
    "df_del1 = df.drop(df[(df['target']=='arms delivery') & (df['label']=='FAVOR')].index)\n",
    "df_del2 = df_del1.drop(df_del1[(df_del1['target']=='npps operation continuation') & (df_del1['label']=='AGAINST')].index)\n",
    "df_del3 = df_del2.drop(df_del2[(df_del2['target']=='speed limit implementation') & (df_del2['label']=='FAVOR')].index)\n",
    "df_del4 = df_del3.drop(df_del3[(df_del3['target']=='ukraine support') & (df_del3['label']=='FAVOR')].index)\n",
    "# Concatenate with downsampled set\n",
    "downsampled = pd.concat([arms_against_downsample, npps_against_downsample, speed_favor_downsample, ua_favor_downsample, df_del4])\n",
    "print('Original dataset: ',df.shape)\n",
    "print('Downsampled final dataset: ',downsampled.shape)\n",
    "# Save in a new csv file to have a final downsampled dataset\n",
    "downsampled.to_csv(\" \", index=False, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unique number of users per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---- LOAD FILE ---- #\n",
    "f_name = \"../data/data_unlabeled/all_24feb31dec/3_predictions/stanceclasses/AD - Against.csv\"\n",
    "file = pd.read_csv(f_name)\n",
    "\n",
    "# ---- SHOW NUMBER OF UNIQUE USERS ---- #\n",
    "user_count = file[\"author_id\"].nunique()\n",
    "print('Unique numbers of users by author_id: ', user_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for text duplicates by text_ID written by same user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "# ---- LOAD FILE ---- #\n",
    "f_name = \" \"\n",
    "file = pd.read_csv(f_name)\n",
    "#file = file.astype({'text_source':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SHOW TEXT DUPLICATES AND THE CORRESPONDING TWEET IDS ---- #\n",
    "texts = file[\"text\"]\n",
    "text_dupl = file[texts.isin(texts[texts.duplicated()])].sort_values(\"text_id\")\n",
    "print('Total number of text duplicates: ',len(text_dupl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SHOW NUMBER OF UNIQUE SAMPLES OF FOUND TEXT DUPLICATES ---- #\n",
    "texts_counts = text_dupl[\"text\"].value_counts() #text_source\n",
    "print('Unique elements in column \"text\" and their counts in descending order')\n",
    "print(\"=\" * 100)\n",
    "print(texts_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results\n",
    "text_counts_df = texts_counts.to_frame().reset_index()\n",
    "text_counts_df.to_csv(\" \", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues = text_dupl['text'].nunique()\n",
    "print('Unique elements in column \"text\" :',uniqueValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CHECK IF SAME USER WROTE TEXT DUPLICATES ---- #\n",
    "author_counts = text_dupl[\"author_id\"].value_counts()\n",
    "print('Unique elements in column \"author_id\" and their counts in descending order')\n",
    "print(\"=\" * 100)\n",
    "print(author_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out meta data\n",
    "#file[file.author_id == 1234567].iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_min2 = [c for c in author_counts if c >= 2]\n",
    "author_counts_min = pd.Series(author_counts_min2) \n",
    "mpl.rcParams['font.size'] = 17.5\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "colors = cmap(np.arange(146))\n",
    "author_counts_min.plot.pie(legend=False, labels=None, colors=colors, autopct=lambda p : '{:.0f}%  ({:,.0f})'.format(p,p * sum(author_counts_min)/100) if p > 3 else None, figsize=(30,20))\n",
    "#plt.style.use('fivethirtyeight')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Tweet Authors', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicates by text_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f_name = \" \"\n",
    "scraped_data = pd.read_csv(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = scraped_data[\"text_id\"]\n",
    "duplics = scraped_data[texts.isin(texts[texts.duplicated()])].sort_values(\"text_id\")\n",
    "print('Total number of text duplicates by text_ID: ',len(duplics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete and keep only first occurence\n",
    "nodupl = scraped_data.drop_duplicates(subset=['text_id'], keep='first')\n",
    "nodupl.to_csv(\" \", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
